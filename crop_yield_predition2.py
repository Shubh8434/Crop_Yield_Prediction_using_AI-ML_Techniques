# -*- coding: utf-8 -*-
"""crop_yield_predition2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JN8_DdLquIw2bUG0UzltUreFfO3Fv_9N
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

df = pd.read_csv('/content/drive/MyDrive/yield_df.csv')

df.head()

df.shape

df = df.drop(['Unnamed: 0'], axis=1)

df.head()

df['Area'].unique()

df['Year'].unique()

df.describe()

df.dtypes

df.isnull().sum()

sns.pairplot(df, diag_kind="hist")

df.head()

df.hist(bins=50, figsize=(20,10))
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8
                                , 17))
sns.countplot(y ='Item', data = df, ax=ax)
plt.savefig('crop_count1.png')
plt.show()

df['Item'].value_counts()[:10].plot(kind='pie')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8
                                , 17))
sns.countplot(y ='Area', data = df, ax=ax)

plt.savefig("countplot_area.png")
# Show the plot
plt.show()

df['Item'].value_counts()[:10].plot(kind='pie')
plt.show()

df.plot(kind='density', subplots=True, layout=(3,3), figsize=(20, 15), sharex=False)
plt.show()

df.plot(kind= 'box' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(20,15))

plt.figure(figsize=(16,5))
plt.subplot(1,2,1)
sns.distplot(df['pesticides_tonnes'])

# Finding the IQR
percentile25 = df['pesticides_tonnes'].quantile(0.25)
percentile75 = df['pesticides_tonnes'].quantile(0.75)
iqr = percentile75 - percentile25
upper_limit = percentile75 + 1.5 * iqr
lower_limit = percentile25 - 1.5 * iqr

new_df_cap = df.copy()

new_df_cap['pesticides_tonnes'] = np.where(
    new_df_cap['pesticides_tonnes'] > upper_limit,
    upper_limit,
    np.where(
        new_df_cap['pesticides_tonnes'] < lower_limit,
        lower_limit,
        new_df_cap['pesticides_tonnes']
    )
)

# Comparing

plt.figure(figsize=(16,8))
plt.subplot(2,2,1)
sns.distplot(df['pesticides_tonnes'])

plt.subplot(2,2,2)
sns.boxplot(df['pesticides_tonnes'])

plt.subplot(2,2,3)
sns.distplot(new_df_cap['pesticides_tonnes'])

plt.subplot(2,2,4)
sns.boxplot(new_df_cap['pesticides_tonnes'])

plt.show()

new_df_cap.plot(kind= 'box' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(20,15))

plt.figure(figsize=(16,5))
plt.subplot(1,2,1)
sns.distplot(df['avg_temp'])

# Finding the IQR
percentile25 = df['avg_temp'].quantile(0.25)
percentile75 = df['avg_temp'].quantile(0.75)

iqr = percentile75 - percentile25
upper_limit = percentile75 + 1.5 * iqr
lower_limit = percentile25 - 1.5 * iqr

new_df_cap = new_df_cap.copy()

new_df_cap['avg_temp'] = np.where(
    new_df_cap['avg_temp'] > upper_limit,
    upper_limit,
    np.where(
        new_df_cap['avg_temp'] < lower_limit,
        lower_limit,
        new_df_cap['avg_temp']
    )
)

# Comparing

plt.figure(figsize=(16,8))
plt.subplot(2,2,1)
sns.distplot(df['avg_temp'])

plt.subplot(2,2,2)
sns.boxplot(df['avg_temp'])

plt.subplot(2,2,3)
sns.distplot(new_df_cap['avg_temp'])

plt.subplot(2,2,4)
sns.boxplot(new_df_cap['avg_temp'])

plt.show()

new_df_cap.plot(kind= 'box' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(20,15))

new_df_cap.head()

new_df_cap.shape

new_df_cap['Item'].unique()

pd.set_option('display.max_columns', None)

new_df_cap['Year'].unique()

new_df_cap['Area'].unique()

y= new_df_cap['hg/ha_yield']
X = new_df_cap.drop(['hg/ha_yield'], axis=1)

X.head()

y.head()

X = X.drop(['Year'], axis=1)

X.head()

X = pd.get_dummies(X, columns = ['Area','Item'])

X.head()

X.shape

X.corr()

import seaborn as sns
#Using Pearson Correlation
plt.figure(figsize=(12,10))
cor = X.corr()
sns.heatmap(cor, annot=True)
plt.show()

# with the following function we can select highly correlated features
# it will remove the first feature that is correlated with anything other feature

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X, 0.05)
len(set(corr_features))

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
X=scaler.fit_transform(X)

X = np.array(X)

X

X.shape

y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)

X_test.shape

y_test.shape

X_train.shape

y_train.shape

from sklearn.ensemble import RandomForestRegressor

 # create regressor object
Random = RandomForestRegressor(n_estimators = 100, random_state = 500, n_jobs=-1, min_samples_leaf = 1)

# fit the regressor with x and y data
Random.fit(X_train, y_train)

Random.score(X_test,y_test)

from sklearn.ensemble import ExtraTreesRegressor
extra = ExtraTreesRegressor()
extra.fit(X_train,y_train)

extra.score(X_test,y_test)

from sklearn.ensemble import BaggingRegressor
bag = BaggingRegressor()
bag.fit(X_train,y_train)

bag.score(X_test,y_test)

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

y_pred_RF = Random.predict(X_test)

mse_RF = mean_squared_error(y_test, y_pred_RF)
mae_RF = mean_absolute_error(y_test, y_pred_RF)
r2 = r2_score(y_test,y_pred_RF)
print('Mean squared error using Random Forest: ', mse_RF)
print('Mean absolute error Using Random Forest: ', mae_RF)
print('R^2 value Using Random Forest: ', r2) #accuracy

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(5,5))
plt.scatter(x=y_test, y=y_pred_RF, c="#7CAE00" ,alpha=0.3)

z = np.polyfit(y_test, y_pred_RF, 1)
p = np.poly1d(z)

plt.plot(y_test, p(y_test), '#F8766D')
plt.ylabel('Predict LogS')
plt.xlabel('Experimental LogS')

y_pred_EXTRA = extra.predict(X_test)

mse_EXTRA = mean_squared_error(y_test,y_pred_EXTRA)
mae_EXTRA = mean_absolute_error(y_test, y_pred_EXTRA)
r2 = r2_score(y_test,y_pred_EXTRA)
print('Mean squared error using Extra Tree Regressor: ', mse_EXTRA)
print('Mean absolute error Using Extra Tree Regressor: ', mae_EXTRA)
print('R^2 value Using extra tree regressor: ', r2) #accuracy

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(5,5))
plt.scatter(x=y_test, y=y_pred_EXTRA, c="#7CAE00" ,alpha=0.3)

z = np.polyfit(y_test, y_pred_EXTRA, 1)
p = np.poly1d(z)

plt.plot(y_test, p(y_test), '#F8766D')
plt.ylabel('Predict LogS')
plt.xlabel('Experimental LogS')

from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop
optimizer = RMSprop(0.001)
model = Sequential()
model.add(Dense(256, input_dim=114, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
#Output layer
model.add(Dense(1, activation='linear'))
model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])
model.summary()

history = model.fit(X_train, y_train, validation_split=0.2, epochs =50)

print(history.history.keys())

from matplotlib import pyplot as plt
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = history.history['mae']
val_acc = history.history['val_mae']
plt.plot(epochs, acc, 'y', label='Training MAE')
plt.plot(epochs, val_acc, 'r', label='Validation MAE')
plt.title('Training and validation MAE')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

############################################
#Predict on test data
predictions = model.predict(X_test[:5])
print("Predicted values are: ", predictions)
print("Real values are: ", y_test[:5])
##############################################

#Comparison with other models..
#Neural network - from the current code
mse_neural, mae_neural = model.evaluate(X_test, y_test)
print('Mean squared error from neural net: ', mse_neural)
print('Mean absolute error from neural net: ', mae_neural)
r2_score(y_test[:5], predictions)

model.evaluate(X_test,y_test)

!pip install keras-tuner --upgrade

import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch

def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 50)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=256,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='linear'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='mean_absolute_error',
        metrics=['mean_absolute_error'])
    return model

tuner = RandomSearch(
    build_model,
    objective='val_mean_absolute_error',
    max_trials=5,
    executions_per_trial=3,
    directory='project',
    project_name='Crop Yield Prediction')

tuner.search_space_summary()

tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))

